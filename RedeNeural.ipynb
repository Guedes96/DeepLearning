{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Implementando Rede Neural problema mnist "
      ],
      "metadata": {
        "id": "bltJ5t77FO8o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "InoW4CQRFMPh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch \n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# definindo a conversão de imagem para tensor\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "#carrega a parte de treino do dataset\n",
        "trainset = datasets.MNIST('./MNIST_data/', download=True, train=True, transform=transform)\n",
        "\n",
        "#cria um buffer para pegar os dados por partes\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "#carrega a parte de validacao do dataset\n",
        "valset = datasets.MNIST('./MNIST_data/', download=True, train=True, transform=transform)\n",
        "\n",
        "#cria um buffer para pegar os dados por partes\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "BATDwMnOG0lO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#conferindo o acesso a base de dados\n",
        "dataiter = iter(trainloader)\n",
        "imagens, etiquetas = dataiter.next()\n",
        "plt.imshow(imagens[0].numpy().squeeze(), cmap='gray_r');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "rzSzYYBzJWIx",
        "outputId": "7b15c5a0-3b8c-4a8c-9d9a-b432858a3340"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(imagens[0].shape) #verificar as dimensoes do tensor de cada imagem\n",
        "print(etiquetas[0].shape) #verificar as dimensoes do tensor de cada etiqueta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHiprw0UKJo3",
        "outputId": "b6b6e620-c3cc-4f36-bc4f-97c099d1ffdb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Modelo(nn.Module):\n",
        "    def __init__(self):\n",
        "       super(Modelo, self).__init__()\n",
        "       self.linear1 = nn.Linear(28*28, 128) #camada de entrada, 784 neuronios que se ligam a 128\n",
        "       self.linear2 = nn.Linear(128, 64) #camada interna 1, 128 neuronios que se ligam a 64\n",
        "       self.linear3 = nn.Linear(64, 10) #camada iterna 2, 64 neuronios que se ligam a 10\n",
        "       #para a camada de saida nao é necessario definir nasa pois so precisamos peagr o output da cama interna 2\n",
        "\n",
        "    def forward(self, X):\n",
        "      X = F.relu(self.linear1(X)) #funcao de ativacao da camada de entrada para a camada interna 1\n",
        "      X = F.relu(self.linear2(X)) #funcao de ativacaao da camada interna 1 para a camada interna 2\n",
        "      X = self.linear3(X) #funcao de ativacao da camada interna 2 para a camada de saida, nesse caso f(x) = x\n",
        "      return F.log_softmax(X, dim=1) #dados utilizados para calcular a perda"
      ],
      "metadata": {
        "id": "wBryTq0qLGFZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def treino(modelo, trainloader, device):\n",
        "\n",
        "    otimizador = optim.SGD(modelo.parameters(), lr=0.01, momentum=0.5) #define a politica de atualizacao dos pesos e das bias\n",
        "    inicio = time() #timer para saber quanto tempo levou o treino\n",
        "\n",
        "    criterio = nn.NLLLoss() #definindo o criterio para calcular a perda\n",
        "    EPOCHS = 10 #numero de epochs que o algoritmo rodara\n",
        "    modelo.train() #ativando o modo de treinamento do modelo\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        perda_acumulada = 0 #inicializacao de perda acumulada da epoch em questao\n",
        "\n",
        "        for imagens, etiquetas in trainloader:\n",
        "\n",
        "            imagens = imagens.view(imagens.shape[0], -1) #converteendo as imagens para \"vetores\" de 28*28 casa para ficarem compativeis com a \n",
        "            otimizador.zero_grad() #zerando os gradientes por conta do ciclo anterior\n",
        "\n",
        "            output = modelo(imagens.to(device)) #colocando os dados do modelo\n",
        "            perda_instantanea = criterio(output, etiquetas.to(device)) #calculando a perda da epoch em questao\n",
        "\n",
        "            perda_instantanea.backward() #back propagation a partir da perda\n",
        "\n",
        "            otimizador.step() #atualizando os pesos e a bias\n",
        "\n",
        "            perda_acumulada += perda_instantanea.item() #atualizacao da perda acumulada\n",
        "\n",
        "        else:\n",
        "            print(\"Epoch {} - perda resultante: {}\".format(epoch+1, perda_acumulada/len(trainloader))) \n",
        "    print(\"\\nTempo de treino (em minutos) =\", (time()-inicio)/60)"
      ],
      "metadata": {
        "id": "saH_BwOVOR8K"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validacao(modelo, valloader, device):\n",
        "    conta_corretas, conta_todas = 0, 0 \n",
        "    for imagem, etiquetas in valloader:\n",
        "        for i in range(len(etiquetas)):\n",
        "            img = imagens[i].view(1, 784)\n",
        "            #desativar o autograd para acelerar a validacao. Grafos computacionais dinamicos tem um custo alto de processamento \n",
        "            with torch.no_grad():\n",
        "                logps = modelo(img.to(device)) #output do modelo em escala logaritmica\n",
        "\n",
        "            ps = torch.exp(logps) #converte output para escala normal(lembrando que é um tensor)\n",
        "            probab = list(ps.cpu().numpy()[0])\n",
        "            etiqueta_pred = probab.index(max(probab)) #converte o tensor em um numero, no caso, o umero que o modelo previu \n",
        "            etiqueta_certa = etiquetas.numpy()[i]\n",
        "\n",
        "            if(etiqueta_certa == etiqueta_pred): #compara a previsao com o valor correto\n",
        "                conta_corretas += 1\n",
        "\n",
        "            conta_todas += 1\n",
        "\n",
        "    print(\"total de imagens testadas =\", conta_todas)\n",
        "    print(\"\\nPrecisao do modelo = {}%\". format(conta_corretas*100/conta_todas))"
      ],
      "metadata": {
        "id": "6geajHQKSFZU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = Modelo()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "modelo.to(device) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1B4XF586xleO",
        "outputId": "fa9bb5e8-7742-4a49-8ba4-68bd6bbc6793"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Modelo(\n",
              "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}